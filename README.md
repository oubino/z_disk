# Overview

## IO

Input:
    - Folder of xyz data for each FOV as a .txt file with also additional features for each localisation
               
Output:
    - Folder of xyz data for each z-disk as a .csv file optionally aligned along the x axis

The output directory will look as follows

```bash
...
├── output
│   ├── datastructures
│   │   └── File_1.parquet
│   ├── images
│   │   └── File_1.npy
│   ├── segmentations
│   │   └── File_1.npy
│   ├── segmented_pointclouds
|   │   └── File_1.csv
│   └── segmented_z_disks
|       ├── File_1_zdisk_0.csv
│       └── File_1_zdisk_1.csv
...
```

Each of the directories will be automatically generated apart from segmentations
This should be generated by Ilastik or another segmentation software and the files should have the same name as the original files containing a segmentation for the image i.e. a pixel level label

## Setup

In a new terminal with conda installed create new environment with required packages

```shell
conda create -n z_disk python=3.11 polars pyarrow scikit-learn matplotlib
conda activate z_disk
pip install open3d
```

Clone this directory to your files and move into the directory

```shell
git clone https://github.com/oubino/z_disks.git
cd z_disks
```

In z_disks folder place all .txt data in a folder called data/

```bash
z_disks
├── README.md
├── data
|   ├── File_1.txt
│   └── File_2.txt
└── scripts
    ├── base.py
    ├── image_and_seg_to_pointcloud.py
    ├── pointcloud_to_image.py
    └── separate_and_align.py
```

Make sure you have activated the correct environment 

```shell
conda activate z_disk 
```

## Scripts

Make sure you have activated the correct environment before running scripts

1. pointcloud_to_image
    - Convert each .txt file to an image

```shell
python scripts/pointcloud_to_image.py [ARGS]
```

The following args are required:
    -o Output folder where all analysis will take place
    -x Name of the x column in the data
    -y Name of the y column in the data
    -z Name of the z column in the data
    -hx Size of histogram in x direction
    -hy Size of histogram in y direction
    -hz Size of histogram in z direction
    -s Delimeter separating the items - currently supported either comma or tab
    -bs Bins or size, should be either bins or size, to specify whether sizes above should be interpreted as the number of bins or the size of each bin

The following args are optional
    -c Name of the channel column in the data 

2. (Optional) visualise
    - Visualiase the .parquet pointcloud data

```shell
python scripts/visualise.py [ARGS]
```

The following arg is required:
    -i Path to the input parquet datastructure to be visualised

INSTRUCTIONS FOR ILASTIK SEGMENTATION

3. image_and_seg_to_pointcloud 
    - Combine each segmentation with the original .txt file to extract the localisations and return data in desired output format

```shell
python scripts/image_and_seg_to_pointcloud.py
```

4. separate_and_align
    - Separate the FOV into each z-disk and optionally align using PCA

```shell
python scripts/separate_and_align.py
```

The following args are optional:
    -a If specified then aligns each z-disk with x axis

5. (Optional) visualise
    - Visualiase the .csv pointcloud data

```shell
python scripts/visualise.py [ARGS]
```

The following arg is required:
    -i Path to the input csv to be visualised

## Example on Janelia data

The following details the commands run, assuming all .txt files in folder called data

```shell
python scripts/pointcloud_to_image.py -x "X Position" -y "Y Position" -z "Z Position" -hx 50 -hy 50 -hz 50 -s tab -bs size
python scripts/visualise.py -i output/datastructures/INSERT_FILE_NAME.parquet
```

Then followed Ilastik pixel classification + object detection workflow https://www.ilastik.org/documentation/pixelclassification/pixelclassification & https://www.ilastik.org/documentation/objects/objects

The segmented numpy files are placed in a output/segmentations, then the following commands are run

```shell
python scripts/image_and_seg_to_pointcloud.py
python scripts/separate_and_align.py
```
